{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pedro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- TASK A -------\n",
      "\n",
      "Number of tweets:  3834\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import tweet_cleaning as tc\n",
    "import naive_bayes as nb\n",
    "\n",
    "\n",
    "def listToString(s): \n",
    "    str1 = \"\" \n",
    "    for ele in s: \n",
    "        str1 += (\" \" + ele)\n",
    "    return str1 \n",
    "\n",
    "def readdata(filename):\n",
    "    tweets = []\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            tweets.append([line.split(\"\t\")[0], line.split(\"\t\")[1], line.split(\"\t\")[2]])\n",
    "    return tweets\n",
    "\n",
    "def getInfoFromTweets(data):\n",
    "    ids = []\n",
    "    scores = []\n",
    "    tweets = []\n",
    "    for line in data:\n",
    "        ids.append(line[0])\n",
    "        scores.append(line[1])\n",
    "        tweets.append(line[2])\n",
    "    return ids, scores, tweets\n",
    "\n",
    "def stripEmojisFromTweets(tweets):\n",
    "    emojis = []\n",
    "    new_tweets = []\n",
    "    for tweet in tweets:\n",
    "        emojis.append(getEmojisFromTweet(tweet))\n",
    "        new_tweets.append(re.sub(\"(:)\\w*(:)\", \"\", tweet))\n",
    "    return emojis, new_tweets\n",
    "\n",
    "def getEmojisFromTweet(tweet):\n",
    "    emoji = []\n",
    "    for i in tweet.split():\n",
    "        if i.startswith(\":\"):\n",
    "            emoji.extend(list(filter(None, i.split(\":\"))))\n",
    "    emoji = list(set(emoji))\n",
    "    emoji = [i for i in emoji if i[0].isalpha()]\n",
    "    return emoji\n",
    "\n",
    "def stripHashtagsFromTweets(tweets):\n",
    "    hashtags = []\n",
    "    new_tweets = []\n",
    "    for tweet in tweets:\n",
    "        hashtagsOfTweet = re.findall(\"#\\w+\", tweet)\n",
    "        hashtags.append(hashtagsOfTweet)\n",
    "        new_tweets.append(\" \".join(filter(lambda x: x[0] != '#', tweet.split())))\n",
    "    return hashtags, new_tweets\n",
    "\n",
    "def cleanTweets(tweets):\n",
    "    new_tweets = []\n",
    "    for tweet in tweets:\n",
    "        new_tweets.append(tc.clean_tweet(tweet))\n",
    "    return new_tweets\n",
    "    \n",
    "data = readdata(\"../Train Data/train_taskA.txt\")\n",
    "\n",
    "ids, scores, tweets = getInfoFromTweets(data)\n",
    "\n",
    "emojis, tweets = stripEmojisFromTweets(tweets)\n",
    "\n",
    "hashtags, tweets  = stripHashtagsFromTweets(tweets)\n",
    "\n",
    "tweets = cleanTweets(tweets)\n",
    "\n",
    "finalData = []\n",
    "\n",
    "print(\"------- TASK A -------\\n\")\n",
    "print(\"Number of tweets: \", len(data))\n",
    "print(\"\\n\")\n",
    "count = 0\n",
    "for tweet in tweets:\n",
    "    finalData.append(tweets[count] + listToString(emojis[count]) + listToString(hashtags[count]))\n",
    "    \"\"\"\n",
    "    print(\"Tweet \", ids[count], \":\\n\")\n",
    "    print(\"\\tTweet's text': \", tweets[count], \"\\n\")\n",
    "    print(\"\\tTweet's score': \", scores[count], \"\\n\")\n",
    "    print(\"\\tTweet's emojis': \", emojis[count], \"\\n\")\n",
    "    print(\"\\tTweet's hashtags': \", hashtags[count], \"\\n\")\n",
    "    print(\"\\tTweet tokenized by words: \", word_tokenize(tweets[count]), \"\\n\")\n",
    "    print(\"\\tTweet tokenized by sentences: \", sent_tokenize(tweets[count]), \"\\n\")\n",
    "    print(\"---------------\\n\")\n",
    "    \"\"\"\n",
    "    #print(input[count], \"\\n\")\n",
    "    count += 1\n",
    "\n",
    "nb.bag_of_words_multi_input(finalData, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
